{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Riley NLP Assessment",
      "provenance": [],
      "collapsed_sections": [
        "2X_z5AVqtTcQ",
        "vaQh9t7LhIWx",
        "rDSvL4TxhLxU",
        "jj_6LCQ3h98a",
        "tJnLc0RYhynW",
        "3uszd_xljelg",
        "uFQiWxY6b7fs",
        "PumpacbA97XK",
        "bh5Z3D-H-ImS",
        "u2WXYww9-WHf",
        "aoyb1VUp-YYs",
        "mCDjqI2G-cJO",
        "H_xkoYeAWqqJ",
        "D3NgBAMOWs2i",
        "ZEeoqW1NW2La",
        "lxGxy1fsvPZi",
        "tFUDtYOlAT5s",
        "47Sz8or_BU11",
        "PocNsZFGAk9H",
        "HV8ye6HnAc9P",
        "8rkK0wegPpRk",
        "4p11hcoofsQP",
        "lgfWpCldCpMo",
        "sRsbmO2xCd5h",
        "U4JIUigJRqEH",
        "tlWP8-JqSCw-",
        "7O8tWMw-UqJs",
        "jaZpiJCbSnCK",
        "pMBanZ_uSnCX",
        "6ZWTBC9zSnCY",
        "WvlZeSDtE3o-",
        "7WGeBvGvE-DU",
        "5Xrgs7wvFA3N",
        "zEdguyxwFBQS",
        "m8KUCvU9FBmn",
        "ZOZoMokcm2kM",
        "eGKUcqVZvJF9",
        "4rSsr3QMnBn_",
        "zMgt6HlHnFBs",
        "Wln_9PBWnFYM",
        "d2FbzgItnFh2",
        "u7NFDokvnLwG",
        "hwr1lu8GzYnx",
        "VJ2c2ngig92u",
        "-fl1qdKS1nhZ"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Description"
      ],
      "metadata": {
        "id": "2X_z5AVqtTcQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task "
      ],
      "metadata": {
        "id": "vaQh9t7LhIWx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a model that takes a sentence and identifies the organization a person works for and that person's name. E.g.:\n",
        "\n",
        "> *I just went for a jog and ran into Ben Fischer who is the chief revenue officer of XYZ company.*\n",
        "\n",
        "\n",
        "Your model should not care about capitalization, punctuation, etc and **should identify \"Ben Fischer\" as a person and \"XYZ company\" as the organization**. Bonus points if you can **capture their role (chief revenue officer) as well**. You will need to **calculate precision & recall as well**. When thinking about this task you should ask yourself **what is a good baseline** to compare your results against."
      ],
      "metadata": {
        "id": "1tB2zWzjBSQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Provided Dataset"
      ],
      "metadata": {
        "id": "rDSvL4TxhLxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TACRED"
      ],
      "metadata": {
        "id": "jj_6LCQ3h98a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TAC Relation Extract Dataset is a large-scale relation extraction dataset with 106,264 examples built over newswire and web text from the corpus used in the yearly TAC Knowledge Base Population (TAC KBP) challenges. Examples in TACRED cover 41 relation types as used in the TAC KBP challenges (e.g., per:schools_attended and org:members) or are labeled as no_relation if no defined relation is held. \n"
      ],
      "metadata": {
        "id": "mqUj_H6REEMO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example from TACRED dev** \n",
        "\n",
        "> Sentence: PER-SUBJ graduated from North Korea's elite Kim II Sung University and ORG-OBJ ORG-OBJ\n",
        "\n",
        "> Labels: *per:schools_attended*"
      ],
      "metadata": {
        "id": "fBFEVwT1jOPt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Literature"
      ],
      "metadata": {
        "id": "tJnLc0RYhynW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zhang et al. (2017)**\n",
        "\n",
        "\n",
        "\n",
        "*   Used a sequence model with an attention mechanism that uses token positions relative to subjects/objects to classify the relation expressed by the entities. \n",
        "*   Achieve 65.1% F1 on TACRED test set, claim that CNN-based models achieve higher precision, RNN recall etc.\n",
        "*   Sentence length negatively correlates with model performance; RNNs are more robust to length\n",
        "*   Visualized model attention, found emphasis on verbs/non-proper nouns critical for expressing relations\n",
        "\n",
        "\n",
        "*   Used entity masking to add entity type information as features, ensures proper nouns are not factors that influence model weights\n",
        "\n",
        "\n",
        "**Lyu and Chen (2021) - SOTA on TACRED**\n",
        "\n",
        "\n",
        "*   Proposed a model-agnostic framework that significantly improves precision on relation classification tasks by restricting candidate relations based on candidate entity types, e.g. given a PER-SUBJ and ORG-OBJ, relations such as per:origin will not be considered during inference.\n",
        "*   Two stages - a simple binary classifier to distinguish non-relational sentences, followed by distinct multi-class classifiers with non-overlapping relation domains, ie. one classifier handles all plausible relations for a given subject-object candidate entity pair\n",
        "*   Improve on F1 established in Zhang et al. (2017) by 10.1 points, without specializing or over-engineering a novel model architecture\n",
        "\n",
        "    - mainly due to an increase in precision upon imposing restrictions on candidate entities; 69.8 --> 88.3\n",
        "*   Use a GCN on dependency relations without candidate restriction as baseline\n",
        "\n",
        "\n",
        "*   Weak negative correlation between false positives and amount of corresponding training data, ie. fewer examples of a relation --> increased false positives of that relation"
      ],
      "metadata": {
        "id": "nm1t6Kl3Bo-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Research Questions"
      ],
      "metadata": {
        "id": "3uszd_xljelg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In my task breakdown, I'll address these research questions:\n",
        "\n",
        "1. How can TACRED be transformed into a dataset through which we can evaluate a solution to the challenge? <br>\n",
        "\n",
        "    - Which examples in TACRED are relevant for the objective, and how can we establish gold labels to quantify performance? <br>\n",
        "\n",
        "2. What metrics can be used to describe the performance of a solution? <br>\n",
        "\n",
        "3. How should I go about establishing a baseline approach? <br>"
      ],
      "metadata": {
        "id": "dNMO4wIDjh-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subtasks"
      ],
      "metadata": {
        "id": "uFQiWxY6b7fs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subtask 1"
      ],
      "metadata": {
        "id": "PumpacbA97XK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Refactor TACRED\n",
        "    - get all the examples that have original ground truth relation labels in:\n",
        "\n",
        "    > org:founded_by, <br> org:shareholders, <br> org:top_members/employees, <br> per:employee_of, <br> per:title <br>\n",
        "\n",
        "    - in the first 3 relation types, the ground truth ORG is denoted by the span between *subj_start* and *subj_end* indices, likewise the ground truth PER, *obj_start* etc. <br>\n",
        "\n",
        "    - in the last 2 relation types, the ground truth ORG is denoted by the span between *obj_start* and *obj_end* indices; PER, *subj*, etc. <br>\n",
        "\n",
        "      - in the *per:title* relation, there is no guarantee that ORG exists in the ground truth - therefore these examples should be further filtered <br>\n",
        "\n",
        "      - for each of the examples ⊂ *per:title*, if the ORG NER tag is found in exactly one token span within the sentence, the example can safely be added to the refactored dataset, with the indices denoting the ORG span used as ground truth <br>\n",
        "\n",
        "    - after looking at examples in the *per:_* relation, I noticed some of the *subj* spans (which are meant to denote a PER, ie proper noun) demarcate pronouns - these examples will be removed from the refactored dataset <br>\n",
        "\n",
        "    - in a more intricate solution, co-referencing techniques could be used to map the pronouns back to the NNP however"
      ],
      "metadata": {
        "id": "NBS__c-h9T5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subtask 2"
      ],
      "metadata": {
        "id": "bh5Z3D-H-ImS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Rationalize and Determine Evaluation Criteria <br>\n",
        "\n",
        "  - because the task is well defined, I will elect to use strict-match criteria to evaluate model performance <br>\n",
        "\n",
        "  - with the exact match criterion, precision/recall → F1 will be calculated at both coarse and fine-grained levels of analysis <br>\n",
        "\n",
        "      - at the entity-level, **to determine which of the two entities are harder to predict**, or the psuedo- sample complexity of the curated dataset <br>\n",
        "      - at the example-level, **to determine the appropriateness of a given model for the task**<br>"
      ],
      "metadata": {
        "id": "UFTjXzMJ9bTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subtask 3"
      ],
      "metadata": {
        "id": "u2WXYww9-WHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Establish a Performance Baseline\n",
        "\n",
        "    - TACRED, as it ships, comes with dependency relation (UD), part-of-speech, and NER tags for each token in every example, based on Stanford's well-cited and performance CoreNLP module <br>\n",
        "\n",
        "      - these tags are easily attainable by many parsers, offered through several frameworks (spacy, textblob, nltk, etc.) → accessing them at any stage in a solution would not comprise the integrity of the research <br>\n",
        "\n",
        "      - therefore, a solution utilising these tags would serve as an appropriate baseline approach, because it <br>\n",
        "      \n",
        "        - is entirely inexpensive <br>\n",
        "\n",
        "        - significantly better than random guesses <br>\n",
        "\n",
        "        - relies on info. that at the bare minimum, a more expensive solution should latently encode or otherwise account for <br>"
      ],
      "metadata": {
        "id": "PPWUSnKE9gKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subtask 4"
      ],
      "metadata": {
        "id": "aoyb1VUp-YYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Reflect, Iterate, and Improve Upon the Baseline <br>\n",
        "    \n",
        "    - after the baseline is established, new approaches should be implemented/tested one-at-a-time, such that each successive iteration <br>\n",
        "    \n",
        "      - is justified by reflecting on its predecessor <br>\n",
        "\n",
        "      - attempts to improve specific areas of weakness, eg. precision of PER entity type extraction <br>"
      ],
      "metadata": {
        "id": "pXcl2BQU9qp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subtask 5"
      ],
      "metadata": {
        "id": "mCDjqI2G-cJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Perform Inference on the Test Set <br>\n",
        "\n",
        "  - with each model/solution iteration"
      ],
      "metadata": {
        "id": "GT4JIKaS9zla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solution"
      ],
      "metadata": {
        "id": "H_xkoYeAWqqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subtask 1 - Refactor TACRED"
      ],
      "metadata": {
        "id": "D3NgBAMOWs2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Setup**"
      ],
      "metadata": {
        "id": "ZEeoqW1NW2La"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "from pprint import pprint\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "%cd '/content/drive/My Drive/Riley/src'\n",
        "\n",
        "with open('data/TACRED/train.json', 'r') as f:\n",
        "  tacred_train = json.load(f)\n",
        "\n",
        "with open('data/TACRED/dev.json', 'r') as f:\n",
        "  tacred_dev = json.load(f)\n",
        "\n",
        "with open('data/TACRED/test.json', 'r') as f:\n",
        "  tacred_test = json.load(f)\n",
        "\n",
        "def read_tacred(ex): # make it easier to read examples from the dataset\n",
        "  print('subject: ', ' '.join(ex['token'][ex['subj_start']:ex['subj_end'] + 1]))\n",
        "  print('object: ', ' '.join(ex['token'][ex['obj_start']:ex['obj_end'] + 1]))\n",
        "  print('relation: ', ex['relation'], '\\n')\n",
        "  pprint(str(' '.join(ex['token'])))"
      ],
      "metadata": {
        "id": "yZ2DhcRUytn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "read_tacred(tacred_train[21404])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDutRB0X4f28",
        "outputId": "0a5c3d5b-848c-40e2-d25e-bba797cf4cea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "subject:  Marco Contiero\n",
            "object:  Greenpeace European Unit\n",
            "relation:  per:employee_of \n",
            "\n",
            "('`` We look forward to the day when the European Commission also puts defence '\n",
            " 'of the public interest before the interests of US agribusiness and its '\n",
            " \"lobbyists in Brussels and at the WTO , '' said Marco Contiero , policy \"\n",
            " 'adviser on GMOs at Greenpeace European Unit .')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code**"
      ],
      "metadata": {
        "id": "lxGxy1fsvPZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_person_from_example(ex): # readability func\n",
        "  return ' '.join([t for t, l in zip(ex['token'], ex['label']) if l == 1])\n",
        "\n",
        "def get_org_from_example(ex): # readability func\n",
        "  return ' '.join([t for t, l in zip(ex['token'], ex['label']) if l == 2])\n",
        "\n",
        "def get_indices_from_per_title_reln(ex): # special case - we can get some examples from per:title\n",
        "  candidate_orgs, seen = [], 0\n",
        "  for idx, ent_type in enumerate(ex['stanford_ner']): \n",
        "    \n",
        "    if ent_type == 'ORGANIZATION' and idx > seen:\n",
        "      b, e = idx, idx\n",
        "      while (e < len(ex['stanford_ner'])):\n",
        "        if ex['stanford_ner'][e] == 'ORGANIZATION':\n",
        "          e += 1\n",
        "          continue\n",
        "        break\n",
        "      candidate_orgs.append((b, e))\n",
        "      seen = e # don't consider the same span twice\n",
        "  \n",
        "  if len(candidate_orgs) == 1: # if there is exactly one organization in the sentence, we can use the example\n",
        "    comp_start, comp_end = candidate_orgs[0]\n",
        "    person_indices = set(range(ex['subj_start'], ex['subj_end']+1)) # person is the subject, but title is the object\n",
        "    org_indices = set(range(comp_start, comp_end)) # so don't use the title\n",
        "    return person_indices, org_indices\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "def convert_tacred(examples): # apply to all splits independently\n",
        "\n",
        "  RELATIONS = set(['org:founded_by', 'org:shareholders', 'org:top_members/employees', 'per:employee_of'])\n",
        "  bad_subjects = set(['he', 'his', 'she', 'mom', 'her']) \n",
        "  task_split = [] # to be pickled\n",
        "  for example in examples:\n",
        "\n",
        "    if example['relation'] not in RELATIONS: # don't care about hard negatives for now\n",
        "      continue\n",
        "\n",
        "    if example['relation'] == 'per:title': # if there is a person, their title, and exactly one ORGANIZATION span \n",
        "                                           # in the example, we can safely use them as ground truths\n",
        "      try:\n",
        "        person_indices, org_indices = get_indices_from_per_title_reln(example)\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "    if example['relation'] == 'per:employee_of': # object is company, subject is person\n",
        "      person_indices = set(range(example['subj_start'], example['subj_end']+1))\n",
        "      org_indices = set(range(example['obj_start'], example['obj_end']+1))\n",
        "\n",
        "    else: # object is person, subject is company\n",
        "      person_indices = set(range(example['obj_start'], example['obj_end']+1))\n",
        "      org_indices = set(range(example['subj_start'], example['subj_end']+1))\n",
        "\n",
        "    new_ex = {\n",
        "        'id' : example['id'],\n",
        "        'relation' : example['relation'],\n",
        "        'token' : example['token'],\n",
        "        'pos' : example['stanford_pos'],\n",
        "        'ner' : example['stanford_ner'],\n",
        "        'dep_head' : [idx - 1 for idx in example['stanford_head']], # convert from 1-based index\n",
        "        'dep_reln' : example['stanford_deprel'],\n",
        "        'per_span' : list(person_indices),\n",
        "        'org_span' : list(org_indices),\n",
        "        # for evaluating the naive token classification approach\n",
        "        'label' : [0 if i not in (person_indices | org_indices) else 1 if i in person_indices else 2 for i in range(len(example['token']))] # target for Token Classification framework\n",
        "    }\n",
        "\n",
        "    # some of the subjects/objects referring to a person are not names, \n",
        "    # remove these examples to strengthen the ethos of ground truth labels\n",
        "    if get_person_from_example(new_ex).lower() in bad_subjects: \n",
        "      continue\n",
        "\n",
        "    task_split.append(new_ex)\n",
        "\n",
        "  return task_split"
      ],
      "metadata": {
        "id": "LTwMYTfIvVqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity checks !\n",
        "\n",
        "task_sanity = convert_tacred(tacred_train[:5])\n",
        "assert tacred_train[0]['token'][tacred_train[0]['obj_start']:tacred_train[0]['obj_end']+1] == [t for t, l in zip(task_sanity[0]['token'], task_sanity[0]['label']) if l == 1]"
      ],
      "metadata": {
        "id": "XRE-Td329xZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# process and serialize the refactoreed dataset\n",
        "\n",
        "train, dev, test = convert_tacred(tacred_train), convert_tacred(tacred_dev), convert_tacred(tacred_test)\n",
        "\n",
        "with open('data/refactored/train.pkl', 'wb') as d:\n",
        "  pickle.dump(train, d)\n",
        "\n",
        "with open('data/refactored/dev.pkl', 'wb') as d:\n",
        "  pickle.dump(dev, d)\n",
        "\n",
        "with open('data/refactored/test.pkl', 'wb') as d:\n",
        "  pickle.dump(test, d)"
      ],
      "metadata": {
        "id": "KCurOhWEJKD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subtask 2 - Evaluation Metrics"
      ],
      "metadata": {
        "id": "tFUDtYOlAT5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code**"
      ],
      "metadata": {
        "id": "47Sz8or_BU11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet seqeval\n",
        "\n",
        "from seqeval.metrics import classification_report\n",
        "from seqeval.scheme import IOB2\n",
        "\n",
        "def schemify(labels, person_span, org_span): # convert from 0, 1, 2 labels to IOB2 format for evaluating naive token class approach\n",
        "  if len(person_span) == 0 or len(org_span) == 0:\n",
        "    return ['O']*len(labels)\n",
        "\n",
        "  formatted_preds, bper, borg = [], min(person_span), min(org_span)\n",
        "  for i, l in enumerate(labels):\n",
        "\n",
        "    if i == bper:\n",
        "      formatted_preds.append('B-PER')\n",
        "    elif i == borg:\n",
        "      formatted_preds.append('B-ORG')\n",
        "    elif l == 1:\n",
        "      formatted_preds.append('I-PER')\n",
        "    elif l == 2:\n",
        "      formatted_preds.append('I-ORG')\n",
        "    else:\n",
        "      formatted_preds.append('O')\n",
        "  return formatted_preds\n",
        "\n",
        "TC_TRUTH_TRAIN = [schemify(ex['label'], set(ex['per_span']), set(ex['org_span'])) for ex in train]\n",
        "TC_TRUTH_DEV = [schemify(ex['label'], set(ex['per_span']), set(ex['org_span'])) for ex in dev]\n",
        "TC_TRUTH_TEST = [schemify(ex['label'], set(ex['per_span']), set(ex['org_span'])) for ex in test]"
      ],
      "metadata": {
        "id": "_FFVG8kRBT2U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8baba22-acce-4acd-e9c2-66e969a628e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |███████▌                        | 10 kB 37.5 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20 kB 43.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 30 kB 41.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40 kB 24.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 43 kB 1.9 MB/s \n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subtask 3 - Naive Token Classification Baseline"
      ],
      "metadata": {
        "id": "PocNsZFGAk9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simplest approach is to predict PERSON and ORGANIZATION spans by considering token NER tags. In case of multiple PERSON or ORGANIZATION spans, the shortest token distance between two candidates will be computed to resolve a decision. <br>\n",
        "\n",
        "This approach requires no training data, so it is applied directly to the dev split for evaluation. "
      ],
      "metadata": {
        "id": "tYLVI8RDB0VW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Setup**"
      ],
      "metadata": {
        "id": "HV8ye6HnAc9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "%cd '/content/drive/My Drive/Riley/src'\n",
        "\n",
        "with open('data/refactored/train.pkl', 'rb') as p:\n",
        "  train = pickle.load(p)\n",
        "\n",
        "with open('data/refactored/dev.pkl', 'rb') as p:\n",
        "  dev = pickle.load(p)\n",
        "\n",
        "with open('data/refactored/test.pkl', 'rb') as p:\n",
        "  test = pickle.load(p)"
      ],
      "metadata": {
        "id": "nq7FEzhMAfpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code**"
      ],
      "metadata": {
        "id": "8rkK0wegPpRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_naive_predictions(examples):\n",
        "  predictions = []\n",
        "  random_guesses = 0\n",
        "  for ex in examples:\n",
        "\n",
        "    # establish candidates\n",
        "    candidate_persons, candidate_orgs, seen = [], [], 0\n",
        "    for idx, ent_type in enumerate(ex['ner']): \n",
        "      \n",
        "      if ent_type == 'PERSON' and idx > seen:\n",
        "        b, e = idx, idx\n",
        "        while (e < len(ex['ner'])):\n",
        "          if ex['ner'][e] == 'PERSON':\n",
        "            e += 1\n",
        "            continue\n",
        "          break\n",
        "        candidate_persons.append((b, e))\n",
        "        seen = e # don't consider the same span twice\n",
        "      \n",
        "      elif ent_type == 'ORGANIZATION' and idx > seen:\n",
        "        b, e = idx, idx\n",
        "        while (e < len(ex['ner'])):\n",
        "          if ex['ner'][e] == 'ORGANIZATION':\n",
        "            e += 1\n",
        "            continue\n",
        "          break\n",
        "        candidate_orgs.append((b, e))\n",
        "        seen = e # don't consider the same span twice\n",
        "\n",
        "    # some data don't have PERSONS or ORGS - we can't make a prediction\n",
        "    if len(candidate_persons) == 0 or len(candidate_orgs) == 0:\n",
        "      predictions.append(schemify([-1]*len(ex['token']), set(), set()))\n",
        "      random_guesses += 1\n",
        "      continue\n",
        "\n",
        "    # making a naive decision based on distance between two candidates\n",
        "    distmatrix = np.zeros((len(candidate_persons), len(candidate_orgs)))\n",
        "    for i, (pb, pe) in enumerate(candidate_persons):\n",
        "      pm = ((pb + pe)/2) # just use average index for speed\n",
        "      for j, (ob, oe) in enumerate(candidate_orgs):\n",
        "        om = ((ob + oe)/2)\n",
        "        distmatrix[i, j] = np.abs(pm - om)\n",
        "\n",
        "    person_idx, org_idx = np.unravel_index(distmatrix.argmin(), distmatrix.shape)\n",
        "    person_start, person_end = candidate_persons[person_idx]\n",
        "    org_start, org_end = candidate_orgs[org_idx]\n",
        "\n",
        "    pred_person_span, pred_org_span = set(range(person_start, person_end)), set(range(org_start, org_end))\n",
        "    pred_labels = [0 if i not in (pred_person_span | pred_org_span) else 1 if i in pred_person_span else 2 for i in range(len(ex['token']))]\n",
        "\n",
        "\n",
        "    predictions.append(schemify(pred_labels, pred_person_span, pred_org_span))\n",
        "  \n",
        "  print(\"did not make a prediction for\", random_guesses, \"examples!\")\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "1WF5wO-Qdzbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluation**"
      ],
      "metadata": {
        "id": "4p11hcoofsQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "naive_preds = get_naive_predictions(dev)"
      ],
      "metadata": {
        "id": "0a-ImXdJl7Bw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53f26d2d-ac1d-4d4f-aba5-a3bc327000a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "did not make a prediction for 175 examples!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check !\n",
        "\n",
        "assert len(naive_preds) == len(TC_TRUTH_DEV)\n",
        "assert type(naive_preds[0][0]) == type(TC_TRUTH_DEV[0][0])"
      ],
      "metadata": {
        "id": "dElfmTMkzJPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(TC_TRUTH_DEV, naive_preds, scheme=IOB2, mode='strict'), '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIBZ4qotf1RL",
        "outputId": "2eae982b-a14d-4f1b-bb0e-8549d7307cbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ORG       0.73      0.59      0.66       919\n",
            "         PER       0.68      0.55      0.60       919\n",
            "\n",
            "   micro avg       0.70      0.57      0.63      1838\n",
            "   macro avg       0.70      0.57      0.63      1838\n",
            "weighted avg       0.70      0.57      0.63      1838\n",
            " \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reflection**"
      ],
      "metadata": {
        "id": "lgfWpCldCpMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Company names are slightly easier to extract than person names <br>\n",
        "\n",
        "- Many of the examples have names that are not recognized by Stanford's NER system - ie, 175/919 or about 20% of the mistakes were because no prediction was made at all <br>\n",
        "\n",
        "- This means that the metrics displayed in classification report **underrepresent** the potential of this approach [to some extent] <br>\n",
        "\n",
        "- Even though the refactored dataset has about 20x less examples in the dev split than TACRED, and our task is simpler/has a smaller domain, the naive approach has set a strong baseline to improve upon"
      ],
      "metadata": {
        "id": "OQQOtrDuC3_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subtask 4 - Improving on the Baseline"
      ],
      "metadata": {
        "id": "sRsbmO2xCd5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Iteration 1 - Strengthening the Naive Approach (Token Classification)"
      ],
      "metadata": {
        "id": "U4JIUigJRqEH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Motivation**"
      ],
      "metadata": {
        "id": "tlWP8-JqSCw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Correlation between dependency relations and entity types**\n",
        "\n",
        "Based on frequencies from the training set, it's clear that *compound* or *nsubj* relations are far more likely to be associated with the PERSON entity, and *compound* or *nmod* etc. for the ORG entity. I hypothesize that PERSON names will tend to start with *nsubj* and span any succeeding *compound* tags, *nmod* etc. for ORGANIZATION names. "
      ],
      "metadata": {
        "id": "AqOvn1FiSI-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def most_common_deprel_for_entity(examples, ent):\n",
        "  dep2frq = {}\n",
        "  for ex in examples:\n",
        "    deps = [dr for i, dr in enumerate(ex['dep_reln']) if i in ex[f\"{ent}_span\"]]\n",
        "    for d in deps:\n",
        "      if d not in dep2frq:\n",
        "        dep2frq[d] = 0\n",
        "      dep2frq[d] += 1\n",
        "  return sorted(dep2frq.items(), key=lambda x:x[1], reverse=True)"
      ],
      "metadata": {
        "id": "ba7xvFhISI-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "per_deprels = most_common_deprel_for_entity(train, 'per')\n",
        "org_deprels = most_common_deprel_for_entity(train, 'org')\n",
        "\n",
        "pprint(per_deprels)\n",
        "print()\n",
        "pprint(org_deprels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7edde1f4-af0a-43f4-be12-6fda0432035e",
        "id": "qZsfHXNgSI-I"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('compound', 3249),\n",
            " ('nsubj', 2016),\n",
            " ('nmod', 395),\n",
            " ('appos', 195),\n",
            " ('conj', 192),\n",
            " ('dobj', 142),\n",
            " ('nmod:poss', 97),\n",
            " ('nsubjpass', 86),\n",
            " ('ROOT', 77),\n",
            " ('dep', 65),\n",
            " ('punct', 18),\n",
            " ('root', 17),\n",
            " ('amod', 14),\n",
            " ('case', 7),\n",
            " ('xcomp', 7),\n",
            " ('det', 7),\n",
            " ('cc', 6),\n",
            " ('ccomp', 5),\n",
            " ('acl:relcl', 3),\n",
            " ('iobj', 3),\n",
            " ('nmod:tmod', 2),\n",
            " ('advcl', 2),\n",
            " ('aux', 1),\n",
            " ('mark', 1)]\n",
            "\n",
            "[('compound', 4117),\n",
            " ('nmod', 1757),\n",
            " ('case', 327),\n",
            " ('nmod:poss', 268),\n",
            " ('nsubj', 198),\n",
            " ('amod', 173),\n",
            " ('conj', 165),\n",
            " ('appos', 132),\n",
            " ('cc', 115),\n",
            " ('dobj', 107),\n",
            " ('det', 97),\n",
            " ('dep', 41),\n",
            " ('punct', 20),\n",
            " ('ROOT', 13),\n",
            " ('nsubjpass', 13),\n",
            " ('ccomp', 8),\n",
            " ('nummod', 8),\n",
            " ('root', 6),\n",
            " ('mark', 3),\n",
            " ('xcomp', 3),\n",
            " ('aux', 2),\n",
            " ('acl:relcl', 2),\n",
            " ('advcl', 2),\n",
            " ('nmod:tmod', 1),\n",
            " ('advmod', 1),\n",
            " ('acl', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Correlation between token positions and entity types**\n",
        "\n",
        "Based on, etc. looks like the PERSON entity is more likely to appear towards the beginning of the sentence, but ORGANIZATION entities have a much smoother/slighter skew towards the beginning.\n",
        "\n",
        "Based on this, we can define a neighborhood function that allows us to give more consideration to tokens earlier in the sentence to predict as entities."
      ],
      "metadata": {
        "id": "EKtHHsAYSI-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def most_common_position_for_entity(examples, ent):\n",
        "  # we can calculate relative positions since each sentence differs in length\n",
        "  # 20 bin corresponds to first 20% of tokens, 40 etc. \n",
        "  pos2frq = {20 : 0, 40 : 0, 60 : 0, 80 : 0, 100 : 0} \n",
        "  for ex in examples:\n",
        "    pos = [int(100*i/len(ex['label'])) for i in ex[f\"{ent}_span\"]]\n",
        "    for p in pos:\n",
        "      if p < 20:\n",
        "        pos2frq[20] += 1\n",
        "        continue\n",
        "      elif p < 40: \n",
        "        pos2frq[40] += 1\n",
        "        continue\n",
        "      elif p < 60:\n",
        "        pos2frq[60] += 1\n",
        "        continue\n",
        "      elif p < 80:\n",
        "        pos2frq[80] += 1\n",
        "        continue\n",
        "      else:\n",
        "        pos2frq[100] += 1\n",
        "\n",
        "  return sorted(pos2frq.items(), key=lambda x:x[1], reverse=True)"
      ],
      "metadata": {
        "id": "FHoCChqVSI-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "per_pos = most_common_position_for_entity(train, 'per')\n",
        "org_pos = most_common_position_for_entity(train, 'org')\n",
        "\n",
        "pprint(per_pos)\n",
        "print()\n",
        "pprint(org_pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a93d45b8-a8f0-430d-c8e9-63c3551421af",
        "id": "mf9KPWnYSI-I"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(20, 2405), (40, 1211), (60, 1164), (80, 1108), (100, 719)]\n",
            "\n",
            "[(40, 1836), (100, 1668), (20, 1510), (80, 1396), (60, 1170)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I just established that dependency relations and token positions may guide entity predictions when no NER tags are given. We can also trivially conclude that POS tags (ie. NN, NNS, NNP, etc.) can inform the prediction as well. \n",
        "\n",
        "\n",
        "In this iteration, the baseline model will be slightly augmented so that it can make a prediction on the 20% of cases it was forced to predict nothing. \n",
        "\n",
        "\n",
        "```\n",
        "# Psuedocode\n",
        "\n",
        "# case A: no NER tags for person (see find_person)\n",
        "\n",
        "  # if it has a span with compounds preceding a nsubj:\n",
        "    # return the span closest to the beginning \n",
        "  # elif it has a nsubj dependency:\n",
        "    # if left tokens are part of noun phrase, include these in span (Treebank: 19280 instances of nsubj (96%) are right-to-left (child precedes parent))\n",
        "    # return the span closest to the beginning\n",
        "  # otherwise:\n",
        "    # return the first contiguous span of NPs in the sentence\n",
        "\n",
        "# case B: no NER tags for company (see find_org)\n",
        "\n",
        "  # if it has a span with nmod preceding compounds:\n",
        "    # return the span closest to the beginning\n",
        "  # elif it has a nmod dependency:\n",
        "    # if right tokens are part of noun phrase, include these in span (Treebank 18551 instances of nmod (94%) are left-to-right (parent precedes child))\n",
        "  # otherwise:\n",
        "    # return the first contiguous span of NPs in the sentence (that are not already classified as person)\n",
        "\n",
        "# case C: no NER tags for either (see predict_without_ent)\n",
        "  # do A then B\n",
        "```"
      ],
      "metadata": {
        "id": "fPF1vtX9STC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Setup**"
      ],
      "metadata": {
        "id": "7O8tWMw-UqJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "%cd '/content/drive/My Drive/Riley/src'\n",
        "\n",
        "with open('data/refactored/train.pkl', 'rb') as p:\n",
        "  train = pickle.load(p)\n",
        "\n",
        "with open('data/refactored/dev.pkl', 'rb') as p:\n",
        "  dev = pickle.load(p)\n",
        "\n",
        "with open('data/refactored/test.pkl', 'rb') as p:\n",
        "  test = pickle.load(p)"
      ],
      "metadata": {
        "id": "9v67C2F0UqJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Code**"
      ],
      "metadata": {
        "id": "jaZpiJCbSnCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_without_ent(ex, candidate_persons, candidate_orgs): # do this when we can't use NER tags\n",
        "  if len(candidate_persons) + len(candidate_orgs) == 0:\n",
        "    per_span = find_person(ex)\n",
        "    org_span = find_org(ex, per_span)\n",
        "  elif len(candidate_persons) == 0:\n",
        "    per_span = find_person(ex)\n",
        "    org_span = set(range(*candidate_orgs[0]))\n",
        "  else:\n",
        "    per_span = set(range(*candidate_persons[0]))\n",
        "    org_span = find_org(ex, per_span)\n",
        "\n",
        "  pred_labels = [0 if i not in (per_span | org_span) else 1 if i in per_span else 2 for i in range(len(ex['token']))]\n",
        "  return schemify(pred_labels, per_span, org_span)\n",
        "\n",
        "def has_compound_nsubj_seq(ex): # helper boolean\n",
        "    return ('compound', 'nsubj') in zip(ex['dep_reln'], ex['dep_reln'][1:])\n",
        "\n",
        "def has_nmod_compound_seq(ex): # helper boolean\n",
        "    return ('nmod', 'compound') in zip(ex['dep_reln'], ex['dep_reln'][1:])\n",
        "\n",
        "def find_person(ex): # returns set of indices with person\n",
        "  \n",
        "  if has_compound_nsubj_seq(ex):\n",
        "    b, e = 0, 0\n",
        "    for idx, dep in enumerate(ex['dep_reln']):\n",
        "      if dep == 'compound':\n",
        "        b, e = idx, idx\n",
        "        while (e < len(ex['dep_reln'])):\n",
        "          if ex['dep_reln'][e] in ['compound', 'nsubj']:\n",
        "            e += 1\n",
        "            continue\n",
        "          break\n",
        "      return set(range(b, e))\n",
        "  \n",
        "  elif 'nsubj' in ex['dep_reln']:\n",
        "    b, e = ex['dep_reln'].index('nsubj'), ex['dep_reln'].index('nsubj')\n",
        "    while(ex['pos'][b].startswith('NN') and b >= 0):\n",
        "      b -= 1\n",
        "    return set(range(b, e))\n",
        "  \n",
        "  else:\n",
        "    b = next(i for i,p in enumerate(ex['pos']) if p.startswith('NN'))\n",
        "    e = b\n",
        "    while (ex['pos'][e].startswith('NN') and e < len(ex['pos'])):\n",
        "      e += 1\n",
        "    return set(range(b, e))\n",
        "\n",
        "def find_org(ex, per_span): # returns set of indices with organization, not overwriting person\n",
        "\n",
        "  if has_nmod_compound_seq(ex):\n",
        "\n",
        "    for idx, dep in enumerate(ex['dep_reln']):\n",
        "      if dep == 'nmod':\n",
        "        b, e = idx, idx\n",
        "        while (e < len(ex['dep_reln'])):\n",
        "          if ex['dep_reln'][e] in ['compound', 'nmod']:\n",
        "            e += 1\n",
        "            continue\n",
        "          break\n",
        "      if b not in per_span and e not in per_span:\n",
        "        return set(range(b, e))\n",
        "      else:\n",
        "        continue\n",
        "  \n",
        "  elif 'nmod' in ex['dep_reln']:\n",
        "    generator = (i for i, d in enumerate(ex['dep_reln']) if d == 'nmod')\n",
        "    b = next(generator)\n",
        "    while (b in per_span):\n",
        "      b = next(generator)\n",
        "    e = b\n",
        "    while (ex['dep_reln'][e] in ['compound', 'nmod'] and e < len(ex['pos'])):\n",
        "      e += 1\n",
        "    return set(range(b, e))\n",
        "\n",
        "  else:\n",
        "    b = next(i for i,p in enumerate(ex['pos']) if p.startswith('NN') and i not in per_span)\n",
        "    e = b\n",
        "    while (ex['pos'][e].startswith('NN') and e < len(ex['pos'])):\n",
        "      e += 1\n",
        "    return set(range(b, e))\n",
        "\n",
        "def get_slightly_less_naive_predictions(examples):\n",
        "  predictions = []\n",
        "  for ex in examples:\n",
        "\n",
        "    # establish candidates\n",
        "    candidate_persons, candidate_orgs, seen = [], [], 0\n",
        "    for idx, ent_type in enumerate(ex['ner']): \n",
        "      \n",
        "      if ent_type == 'PERSON' and idx > seen:\n",
        "        b, e = idx, idx\n",
        "        while (e < len(ex['ner'])):\n",
        "          if ex['ner'][e] == 'PERSON':\n",
        "            e += 1\n",
        "            continue\n",
        "          break\n",
        "        candidate_persons.append((b, e))\n",
        "        seen = e # don't consider the same span twice\n",
        "      \n",
        "      elif ent_type == 'ORGANIZATION' and idx > seen:\n",
        "        b, e = idx, idx\n",
        "        while (e < len(ex['ner'])):\n",
        "          if ex['ner'][e] == 'ORGANIZATION':\n",
        "            e += 1\n",
        "            continue\n",
        "          break\n",
        "        candidate_orgs.append((b, e))\n",
        "        seen = e # don't consider the same span twice\n",
        "\n",
        "    # some data don't have PERSONS or ORGS - let's make an educated guess this time\n",
        "    if len(candidate_persons) == 0 or len(candidate_orgs) == 0:\n",
        "      predictions.append(predict_without_ent(ex, candidate_persons, candidate_orgs))\n",
        "      continue\n",
        "\n",
        "    # making a naive decision based on distance between two candidates\n",
        "    distmatrix = np.zeros((len(candidate_persons), len(candidate_orgs)))\n",
        "    for i, (pb, pe) in enumerate(candidate_persons):\n",
        "      pm = ((pb + pe)/2) # just use average index \n",
        "      for j, (ob, oe) in enumerate(candidate_orgs):\n",
        "        om = ((ob + oe)/2)\n",
        "        distmatrix[i, j] = np.abs(pm - om)\n",
        "\n",
        "    person_idx, org_idx = np.unravel_index(distmatrix.argmin(), distmatrix.shape)\n",
        "    person_start, person_end = candidate_persons[person_idx]\n",
        "    org_start, org_end = candidate_orgs[org_idx]\n",
        "\n",
        "    pred_person_span, pred_org_span = set(range(person_start, person_end)), set(range(org_start, org_end))\n",
        "    pred_labels = [0 if i not in (pred_person_span | pred_org_span) else 1 if i in pred_person_span else 2 for i in range(len(ex['token']))]\n",
        "\n",
        "    predictions.append(schemify(pred_labels, pred_person_span, pred_org_span))\n",
        "  \n",
        "  return predictions"
      ],
      "metadata": {
        "id": "HtHXOuYCSnCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Evaluation**"
      ],
      "metadata": {
        "id": "pMBanZ_uSnCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "less_naive_preds = get_slightly_less_naive_predictions(dev)"
      ],
      "metadata": {
        "id": "qNAF8CVJSnCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check !\n",
        "\n",
        "assert len(less_naive_preds) == len(TC_TRUTH_DEV)\n",
        "assert type(less_naive_preds[0][0]) == type(TC_TRUTH_DEV[0][0])"
      ],
      "metadata": {
        "id": "gtHLlqBUSnCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(TC_TRUTH_DEV, less_naive_preds, scheme=IOB2, mode='strict'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d030cda2-6fc7-4588-ac0d-58441d68bbc6",
        "id": "Rs1rYUCfSnCX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ORG       0.65      0.64      0.64       919\n",
            "         PER       0.67      0.63      0.65       919\n",
            "\n",
            "   micro avg       0.66      0.63      0.65      1838\n",
            "   macro avg       0.66      0.63      0.65      1838\n",
            "weighted avg       0.66      0.63      0.65      1838\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Reflection**"
      ],
      "metadata": {
        "id": "6ZWTBC9zSnCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Adding rules to ensure a guess is made for each entity in each sample increased our recall - this was expected since recall is the proportion of docs in the corpus that were predicted as positive, and by not attempting to predict 175/919 instances, we were limiting that performance aspect.\n",
        "\n",
        "- Since our precision decreased, we can infer that our dependency/token-based heuristics were too weak/made poor guesses for those 175 hard examples. We could strengthen the rules, but at this point leveraging transfer learning would be more simpler and more effective."
      ],
      "metadata": {
        "id": "1LijVvlJSnCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Iteration 2 - Fine-tuning BERT for Relation Extraction \n"
      ],
      "metadata": {
        "id": "WvlZeSDtE3o-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Setup**"
      ],
      "metadata": {
        "id": "7WGeBvGvE-DU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!pip install -U pip setuptools wheel\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_trf\n",
        "!pip install spacy transformers\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from google.colab import drive\n",
        "from spacy.tokens import Span, DocBin, Doc\n",
        "from spacy.vocab import Vocab\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "import spacy\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "%cd '/content/drive/My Drive/Riley/src'\n",
        "\n",
        "with open('data/refactored/train.pkl', 'rb') as p:\n",
        "  train = pickle.load(p)\n",
        "\n",
        "with open('data/refactored/dev.pkl', 'rb') as p:\n",
        "  dev = pickle.load(p)\n",
        "\n",
        "with open('data/refactored/test.pkl', 'rb') as p:\n",
        "  test = pickle.load(p)\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "LABELS = ['PERSON_AND_COMPANY']\n",
        "\n",
        "%env TCMALLOC_LARGE_ALLOC_REPORT_THRESHOLD=5368709120"
      ],
      "metadata": {
        "id": "UjgvAaJjObBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Code**"
      ],
      "metadata": {
        "id": "5Xrgs7wvFA3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_span2entity(ex):\n",
        "  span2entity, seen = {}, 0\n",
        "  for idx, ent_type in enumerate(ex['ner']): \n",
        "    if ent_type != 'O' and idx > seen:\n",
        "      b, e = idx, idx\n",
        "      while (e < len(ex['ner']) and ex['ner'][e] == ent_type):\n",
        "        e += 1\n",
        "      span2entity[(b, e)] = ent_type\n",
        "      seen = e # don't consider the same span twice\n",
        "  return span2entity\n",
        "\n",
        "def convert_to_spacy(dataset, outf):\n",
        "  misses = 0\n",
        "  Doc.set_extension('rel', default={}, force=True)\n",
        "  vocab = Vocab()\n",
        "\n",
        "  docs, ids = [], set()\n",
        "\n",
        "  for ex in dataset:\n",
        "\n",
        "    span_starts, entities, relations = set(), [], {}\n",
        "    s2e = get_span2entity(ex)\n",
        "    neg, pos = 0, 0\n",
        "    doc = Doc(nlp.vocab, words=ex['token'])\n",
        "\n",
        "    # Parse the GGP entities\n",
        "    seen = 0\n",
        "    for (sb, se), ent in s2e.items():\n",
        "      name = ' '.join([t for i,t in enumerate(ex['token']) if i in set(range(sb, se))])\n",
        "      if seen == 0:\n",
        "        start, end = doc.text.index(name), doc.text.index(name) + len(name)\n",
        "      else:\n",
        "        start, end = doc.text[seen:].index(name) + seen, doc.text[seen:].index(name) + seen + len(name)\n",
        "      seen = end\n",
        "      entity = doc.char_span(start, end, label=ent)\n",
        "      if entity is not None:\n",
        "        entities.append(entity)\n",
        "        span_starts.add(sb)\n",
        "      else:\n",
        "        misses += 1\n",
        "    doc.ents = entities\n",
        "\n",
        "    # Parse the Relations\n",
        "    for s1 in span_starts:\n",
        "      for s2 in span_starts:\n",
        "        relations[(s1, s2)] = {}\n",
        "        if s1 == min(ex['per_span']) and s2 == min(ex['org_span']):\n",
        "          relations[(s1, s2)]['PERSON_AND_COMPANY'] = 1.0\n",
        "        else:\n",
        "          relations[(s1, s2)]['PERSON_AND_COMPANY'] = 0.0\n",
        "    doc._.rel = relations\n",
        "\n",
        "    if len(doc.ents) > 1:\n",
        "      docs.append(doc)\n",
        "\n",
        "  print(misses)\n",
        "  docbin = DocBin(docs=docs, store_user_data=True)\n",
        "  docbin.to_disk(outf)"
      ],
      "metadata": {
        "id": "Um5vMrIrO1X-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_to_spacy(train, 'data/train.spacy')\n",
        "convert_to_spacy(dev, 'data/dev.spacy')\n",
        "convert_to_spacy(test, 'data/test.spacy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSbp4oHcO--e",
        "outputId": "039c9f53-7919-455c-a681-6188e98624e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "1\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Evaluation**"
      ],
      "metadata": {
        "id": "zEdguyxwFBQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env TRF_PATH=\"philschmid/distilroberta-base-ner-conll2003\"\n",
        "%env MODEL_STRING =\"distilroberta-base-ner-conll2003\"\n",
        "%env TRAIN_BIN=\"train.spacy\"\n",
        "%env DEV_BIN=\"dev.spacy\"\n",
        "%env TEST_BIN=\"test.spacy\"\n",
        "\n",
        "!spacy project run train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLvg4bBuHcL_",
        "outputId": "30b8bbad-3e76-4cc1-e505-f4fde97e32bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TRF_PATH=\"philschmid/distilroberta-base-ner-conll2003\"\n",
            "env: MODEL_STRING=\"distilroberta-base-ner-conll2003\"\n",
            "env: TRAIN_BIN=\"train.spacy\"\n",
            "env: DEV_BIN=\"dev.spacy\"\n",
            "env: TEST_BIN=\"test.spacy\"\n",
            "\u001b[1m\n",
            "=================================== train ===================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy train configs/rel_trf.cfg --output models/distilroberta-base-ner-conll2003 --components.transformer.model.name philschmid/distilroberta-base-ner-conll2003 --paths.train data/train.spacy --paths.dev data/dev.spacy -c ./scripts/custom_functions.py --gpu-id 0\n",
            "\u001b[38;5;4mℹ Saving to output directory:\n",
            "models/distilroberta-base-ner-conll2003\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-06-20 13:12:52,176] [INFO] Set up nlp object from config\n",
            "[2022-06-20 13:12:52,189] [INFO] Pipeline: ['transformer', 'relation_extractor']\n",
            "[2022-06-20 13:12:52,195] [INFO] Created vocabulary\n",
            "[2022-06-20 13:12:52,196] [INFO] Finished initializing nlp object\n",
            "Some weights of the model checkpoint at philschmid/distilroberta-base-ner-conll2003 were not used when initializing RobertaModel: ['classifier.weight', 'classifier.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at philschmid/distilroberta-base-ner-conll2003 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[2022-06-20 13:13:01,497] [INFO] Initialized pipeline components: ['transformer', 'relation_extractor']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'relation_extractor']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS RELAT...  REL_MICRO_P  REL_MICRO_R  REL_MICRO_F  SCORE \n",
            "---  ------  -------------  -------------  -----------  -----------  -----------  ------\n",
            "  0       0           0.07           0.28         3.51        50.19         6.56    0.07\n",
            "  0     100           4.11           6.76        46.91        81.32        59.50    0.60\n",
            "  0     200           0.52           2.15        48.20        88.52        62.41    0.62\n",
            "  1     300           0.31           1.95        82.37        55.45        66.28    0.66\n",
            "  1     400           0.25           1.82        88.46        35.80        50.97    0.51\n",
            "  2     500           0.25           1.56        68.30        78.79        73.17    0.73\n",
            "  2     600           0.19           1.40        71.74        76.07        73.84    0.74\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "models/distilroberta-base-ner-conll2003/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env TRF_PATH=\"deepset/minilm-uncased-squad2\"\n",
        "%env MODEL_STRING = \"minilm-uncased-squad2\"\n",
        "%env TRAIN_BIN=\"train.spacy\"\n",
        "%env DEV_BIN=\"dev.spacy\"\n",
        "%env TEST_BIN=\"test.spacy\"\n",
        "\n",
        "!spacy project run train "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5exZCBiY5dq",
        "outputId": "70ca91ae-58b1-4e7d-934e-7a22d19e7c5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TRF_PATH=\"deepset/minilm-uncased-squad2\"\n",
            "env: MODEL_STRING=\"minilm-uncased-squad2\"\n",
            "env: TRAIN_BIN=\"train.spacy\"\n",
            "env: DEV_BIN=\"dev.spacy\"\n",
            "env: TEST_BIN=\"test.spacy\"\n",
            "\u001b[1m\n",
            "=================================== train ===================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy train configs/rel_trf.cfg --output models/minilm-uncased-squad2 --components.transformer.model.name deepset/minilm-uncased-squad2 --paths.train data/train.spacy --paths.dev data/dev.spacy -c ./scripts/custom_functions.py --gpu-id 0\n",
            "\u001b[38;5;4mℹ Saving to output directory: models/minilm-uncased-squad2\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-06-20 13:23:43,015] [INFO] Set up nlp object from config\n",
            "[2022-06-20 13:23:43,026] [INFO] Pipeline: ['transformer', 'relation_extractor']\n",
            "[2022-06-20 13:23:43,031] [INFO] Created vocabulary\n",
            "[2022-06-20 13:23:43,033] [INFO] Finished initializing nlp object\n",
            "Downloading: 100% 107/107 [00:00<00:00, 66.3kB/s]\n",
            "Downloading: 100% 477/477 [00:00<00:00, 371kB/s]\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 716kB/s]\n",
            "Downloading: 100% 112/112 [00:00<00:00, 79.3kB/s]\n",
            "Downloading: 100% 127M/127M [00:02<00:00, 49.8MB/s]\n",
            "Some weights of the model checkpoint at deepset/minilm-uncased-squad2 were not used when initializing BertModel: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2022-06-20 13:23:56,818] [INFO] Initialized pipeline components: ['transformer', 'relation_extractor']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'relation_extractor']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS RELAT...  REL_MICRO_P  REL_MICRO_R  REL_MICRO_F  SCORE \n",
            "---  ------  -------------  -------------  -----------  -----------  -----------  ------\n",
            "  0       0           0.06           0.67         3.62       100.00         6.99    0.07\n",
            "  0     100           7.76          17.67         0.00         0.00         0.00    0.00\n",
            "  0     200           0.62           2.36        55.00        81.32        65.62    0.66\n",
            "  1     300           0.37           1.83        78.69        65.37        71.41    0.71\n",
            "  1     400           0.28           1.68        77.85        67.70        72.42    0.72\n",
            "  2     500           0.24           1.42        72.64        71.79        72.21    0.72\n",
            "  2     600           0.20           1.34        78.68        67.51        72.67    0.73\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "models/minilm-uncased-squad2/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Reflection**"
      ],
      "metadata": {
        "id": "m8KUCvU9FBmn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- improved upon the dev set baseline F1 by about **11 percentage points**, this would seem great but it's a really small set of data, so it should not be interpreted as a major success <br>\n",
        "\n",
        "- even though SQUAD 2 is a harder pre-training objective than conll NER, the difference in performance was not that significant <br>\n",
        "\n",
        "- there could be two things holding our model back: <br>\n",
        "\n",
        "  - did not train with labels for other relations <br>\n",
        "\n",
        "  - relying on given NER tags, which were not super useful according to our baseline analysis <br>"
      ],
      "metadata": {
        "id": "jPsCJatyudTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Iteration 3 - Fine-tuning with Adversarial Examples"
      ],
      "metadata": {
        "id": "ZOZoMokcm2kM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Motivation**"
      ],
      "metadata": {
        "id": "eGKUcqVZvJF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the last iteration, I constructed a training set by only assigning '1.0' to the desired person/company relations, while assigning 0 to all other relations. In this iteration, I will take examples from relation categories that might carry overlap with the categories we defined as holding person/company relations, so the model can become more discriminative in confusing contexts."
      ],
      "metadata": {
        "id": "1jSGIwibvMTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Re-refactoring the Dataset\n",
        "\n",
        "%%capture\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "from pprint import pprint\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "%cd '/content/drive/My Drive/Riley/src'\n",
        "\n",
        "with open('data/TACRED/train.json', 'r') as f:\n",
        "  tacred_train = json.load(f)\n",
        "\n",
        "with open('data/TACRED/dev.json', 'r') as f:\n",
        "  tacred_dev = json.load(f)\n",
        "\n",
        "with open('data/TACRED/test.json', 'r') as f:\n",
        "  tacred_test = json.load(f)\n",
        "\n",
        "def get_person_from_example(ex): # readability func\n",
        "  return ' '.join([t for t, l in zip(ex['token'], ex['label']) if l == 1])\n",
        "\n",
        "def get_org_from_example(ex): # readability func\n",
        "  return ' '.join([t for t, l in zip(ex['token'], ex['label']) if l == 2])\n",
        "\n",
        "def get_indices_from_per_title_reln(ex): # special case - we can get some examples from per:title\n",
        "  candidate_orgs, seen = [], 0\n",
        "  for idx, ent_type in enumerate(ex['stanford_ner']): \n",
        "    \n",
        "    if ent_type == 'ORGANIZATION' and idx > seen:\n",
        "      b, e = idx, idx\n",
        "      while (e < len(ex['stanford_ner'])):\n",
        "        if ex['stanford_ner'][e] == 'ORGANIZATION':\n",
        "          e += 1\n",
        "          continue\n",
        "        break\n",
        "      candidate_orgs.append((b, e))\n",
        "      seen = e # don't consider the same span twice\n",
        "  \n",
        "  if len(candidate_orgs) == 1: # if there is exactly one organization in the sentence, we can use the example\n",
        "    comp_start, comp_end = candidate_orgs[0]\n",
        "    person_indices = set(range(ex['subj_start'], ex['subj_end']+1)) # person is the subject, but title is the object\n",
        "    org_indices = set(range(comp_start, comp_end)) # so don't use the title\n",
        "    return person_indices, org_indices\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "def convert_tacred_with_negatives(examples): # apply to all splits independently\n",
        "\n",
        "  POSITIVE = set(['org:founded_by', 'org:shareholders', 'org:top_members/employees', 'per:employee_of', 'per:title'])\n",
        "  NEGATIVE = set(['org:member_of', 'org:members', 'per:schools_attended', 'per:origin'])\n",
        "\n",
        "  bad_subjects = set(['he', 'his', 'she', 'mom', 'her']) \n",
        "  task_split = [] # to be pickled\n",
        "  for example in examples:\n",
        "\n",
        "    if example['relation'] in POSITIVE:\n",
        "\n",
        "      if example['relation'] == 'per:title': # if there is a person, their title, and exactly one ORGANIZATION span \n",
        "                                            # in the example, we can safely use them as ground truths\n",
        "        try:\n",
        "          person_indices, org_indices = get_indices_from_per_title_reln(example)\n",
        "        except:\n",
        "          continue\n",
        "\n",
        "      if example['relation'] == 'per:employee_of': # object is company, subject is person\n",
        "        person_indices = set(range(example['subj_start'], example['subj_end']+1))\n",
        "        org_indices = set(range(example['obj_start'], example['obj_end']+1))\n",
        "\n",
        "      else: # object is person, subject is company\n",
        "        person_indices = set(range(example['obj_start'], example['obj_end']+1))\n",
        "        org_indices = set(range(example['subj_start'], example['subj_end']+1))\n",
        "\n",
        "      new_ex = {\n",
        "          'id' : example['id'],\n",
        "          'relation' : example['relation'],\n",
        "          'token' : example['token'],\n",
        "          'pos' : example['stanford_pos'],\n",
        "          'ner' : example['stanford_ner'],\n",
        "          'dep_head' : [idx - 1 for idx in example['stanford_head']], # convert from 1-based index\n",
        "          'dep_reln' : example['stanford_deprel'],\n",
        "          'per_span' : list(person_indices),\n",
        "          'org_span' : list(org_indices),\n",
        "          # for evaluating the naive token classification approach\n",
        "          'label' : [0 if i not in (person_indices | org_indices) else 1 if i in person_indices else 2 for i in range(len(example['token']))] # target for Token Classification framework\n",
        "      }\n",
        "\n",
        "    elif example['relation'] in NEGATIVE:\n",
        "\n",
        "      if example['relation'].startswith('org'):\n",
        "        person_indices = set(range(example['obj_start'], example['obj_end']+1))\n",
        "        org_indices = set(range(example['subj_start'], example['subj_end']+1))\n",
        "      else:\n",
        "        person_indices = set(range(example['subj_start'], example['subj_end']+1))\n",
        "        org_indices = set(range(example['obj_start'], example['obj_end']+1))\n",
        "\n",
        "      new_ex = {\n",
        "          'id' : example['id'],\n",
        "          'relation' : example['relation'],\n",
        "          'token' : example['token'],\n",
        "          'pos' : example['stanford_pos'],\n",
        "          'ner' : example['stanford_ner'],\n",
        "          'dep_head' : [idx - 1 for idx in example['stanford_head']], # convert from 1-based index\n",
        "          'dep_reln' : example['stanford_deprel'],\n",
        "          'per_span' : list(person_indices),\n",
        "          'org_span' : list(org_indices),\n",
        "          # for evaluating the naive token classification approach\n",
        "          'label' : [0 if i not in (person_indices | org_indices) else 1 if i in person_indices else 2 for i in range(len(example['token']))] # target for Token Classification framework\n",
        "      }\n",
        "\n",
        "    else:\n",
        "      continue\n",
        "      \n",
        "    # some of the subjects/objects referring to a person are not names, \n",
        "    # remove these examples to strengthen the ethos of ground truth labels\n",
        "    if get_person_from_example(new_ex).lower() in bad_subjects: \n",
        "      continue\n",
        "\n",
        "    task_split.append(new_ex)\n",
        "\n",
        "  return task_split"
      ],
      "metadata": {
        "id": "_d0u32iayWK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# process and serialize the re-refactored dataset\n",
        "\n",
        "train, dev, test = convert_tacred_with_negatives(tacred_train), convert_tacred_with_negatives(tacred_dev), convert_tacred_with_negatives(tacred_test)\n",
        "\n",
        "with open('data/with_negatives/train.pkl', 'wb') as d:\n",
        "  pickle.dump(train, d)\n",
        "\n",
        "with open('data/with_negatives/dev.pkl', 'wb') as d:\n",
        "  pickle.dump(dev, d)\n",
        "\n",
        "with open('data/with_negatives/test.pkl', 'wb') as d:\n",
        "  pickle.dump(test, d)"
      ],
      "metadata": {
        "id": "c64h7bL0yjei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Setup**"
      ],
      "metadata": {
        "id": "4rSsr3QMnBn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!pip install -U pip setuptools wheel\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_trf\n",
        "!pip install spacy transformers\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from google.colab import drive\n",
        "from spacy.tokens import Span, DocBin, Doc\n",
        "from spacy.vocab import Vocab\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "import spacy\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "%cd '/content/drive/My Drive/Riley/src'\n",
        "\n",
        "with open('data/with_negatives/train.pkl', 'rb') as p:\n",
        "  train = pickle.load(p)\n",
        "\n",
        "with open('data/with_negatives/dev.pkl', 'rb') as p:\n",
        "  dev = pickle.load(p)\n",
        "\n",
        "with open('data/with_negatives/test.pkl', 'rb') as p:\n",
        "  test = pickle.load(p)\n",
        "\n",
        "LABELS = ['PERSON_AND_COMPANY']\n",
        "POSITIVE = set(['org:founded_by', 'org:shareholders', 'org:top_members/employees', 'per:employee_of', 'per:title'])\n",
        "# NEGATIVE = set(['org:member_of', 'org:members', 'per:schools_attended', 'per:origin'])\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "%env TCMALLOC_LARGE_ALLOC_REPORT_THRESHOLD=5368709120"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nipwg3xpypML",
        "outputId": "22583237-c0e2-496a-e236-2b3941f435aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TCMALLOC_LARGE_ALLOC_REPORT_THRESHOLD=5368709120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Code**"
      ],
      "metadata": {
        "id": "zMgt6HlHnFBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_span2entity(ex):\n",
        "  span2entity, seen = {}, 0\n",
        "  for idx, ent_type in enumerate(ex['ner']): \n",
        "    if ent_type != 'O' and idx > seen:\n",
        "      b, e = idx, idx\n",
        "      while (e < len(ex['ner']) and ex['ner'][e] == ent_type):\n",
        "        e += 1\n",
        "      span2entity[(b, e)] = ent_type\n",
        "      seen = e # don't consider the same span twice\n",
        "  return span2entity\n",
        "\n",
        "def convert_to_spacy_with_adversarial_examples(dataset, outf):\n",
        "  misses = count = 0\n",
        "  Doc.set_extension('rel', default={}, force=True)\n",
        "  vocab = Vocab()\n",
        "\n",
        "  docs, ids = [], set()\n",
        "\n",
        "  for ex in dataset:\n",
        "\n",
        "    span_starts, entities, relations = set(), [], {}\n",
        "    s2e = get_span2entity(ex)\n",
        "    neg, pos = 0, 0\n",
        "    doc = Doc(nlp.vocab, words=ex['token'])\n",
        "\n",
        "    # Parse the GGP entities\n",
        "    seen = 0\n",
        "    for (sb, se), ent in s2e.items():\n",
        "      name = ' '.join([t for i,t in enumerate(ex['token']) if i in set(range(sb, se))])\n",
        "      if seen == 0:\n",
        "        start, end = doc.text.index(name), doc.text.index(name) + len(name)\n",
        "      else:\n",
        "        start, end = doc.text[seen:].index(name) + seen, doc.text[seen:].index(name) + seen + len(name)\n",
        "      seen = end\n",
        "      entity = doc.char_span(start, end, label=ent)\n",
        "      if entity is not None:\n",
        "        entities.append(entity)\n",
        "        span_starts.add(sb)\n",
        "      else:\n",
        "        misses += 1\n",
        "    doc.ents = entities\n",
        "\n",
        "    # Parse the Relations\n",
        "    for s1 in span_starts:\n",
        "      for s2 in span_starts:\n",
        "        relations[(s1, s2)] = {}\n",
        "        if s1 == min(ex['per_span']) and s2 == min(ex['org_span']):\n",
        "          if ex['relation'] in POSITIVE:\n",
        "            relations[(s1, s2)]['PERSON_AND_COMPANY'] = 1.0\n",
        "          else:\n",
        "            relations[(s1, s2)]['PERSON_AND_COMPANY'] = 0.5 # the entities are person and org, but the relation may or may not exist\n",
        "                                                            # we lack ground truth data, so make it ambiguous\n",
        "        else:\n",
        "          relations[(s1, s2)]['PERSON_AND_COMPANY'] = 0.0\n",
        "\n",
        "          # if ex['relation'] in NEGATIVE:\n",
        "          #   relations[(s1, s2)]['OTHER_RELATION'] = 1.0\n",
        "          # else:\n",
        "          #   relations[(s1, s2)]['OTHER_RELATION'] = 0.0\n",
        "    doc._.rel = relations\n",
        "\n",
        "    if len(doc.ents) > 1:\n",
        "      docs.append(doc)\n",
        "      count += 1\n",
        "\n",
        "  print(misses)\n",
        "  print(count)\n",
        "  docbin = DocBin(docs=docs, store_user_data=True)\n",
        "  docbin.to_disk(outf)"
      ],
      "metadata": {
        "id": "PQ04eJBExvse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_to_spacy_with_adversarial_examples(train, 'data/train_withnegatives.spacy')"
      ],
      "metadata": {
        "id": "mmmFbyTMwbUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Evaluation**"
      ],
      "metadata": {
        "id": "Wln_9PBWnFYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env TRF_PATH=\"philschmid/distilroberta-base-ner-conll2003\"\n",
        "%env MODEL_STRING =\"distilroberta-base-ner-conll2003_withnegatives\"\n",
        "%env TRAIN_BIN=\"train_withnegatives.spacy\"\n",
        "%env DEV_BIN=\"dev.spacy\"\n",
        "%env TEST_BIN=\"test.spacy\"\n",
        "\n",
        "!spacy project run train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPs2N6WYJEE4",
        "outputId": "d4f798c1-76a6-47ed-dd5f-33ef6bd23e6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TRF_PATH=\"philschmid/distilroberta-base-ner-conll2003\"\n",
            "env: MODEL_STRING=\"distilroberta-base-ner-conll2003_withnegatives\"\n",
            "env: TRAIN_BIN=\"train_withnegatives.spacy\"\n",
            "env: DEV_BIN=\"dev.spacy\"\n",
            "env: TEST_BIN=\"test.spacy\"\n",
            "\u001b[1m\n",
            "=================================== train ===================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy train configs/rel_trf.cfg --output models/distilroberta-base-ner-conll2003_withnegatives --components.transformer.model.name philschmid/distilroberta-base-ner-conll2003 --paths.train data/train_withnegatives.spacy --paths.dev data/dev.spacy -c ./scripts/custom_functions.py --gpu-id 0\n",
            "\u001b[38;5;2m✔ Created output directory:\n",
            "models/distilroberta-base-ner-conll2003_withnegatives\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory:\n",
            "models/distilroberta-base-ner-conll2003_withnegatives\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-06-20 13:49:15,154] [INFO] Set up nlp object from config\n",
            "[2022-06-20 13:49:15,166] [INFO] Pipeline: ['transformer', 'relation_extractor']\n",
            "[2022-06-20 13:49:15,170] [INFO] Created vocabulary\n",
            "[2022-06-20 13:49:15,172] [INFO] Finished initializing nlp object\n",
            "Some weights of the model checkpoint at philschmid/distilroberta-base-ner-conll2003 were not used when initializing RobertaModel: ['classifier.weight', 'classifier.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at philschmid/distilroberta-base-ner-conll2003 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[2022-06-20 13:49:26,080] [INFO] Initialized pipeline components: ['transformer', 'relation_extractor']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'relation_extractor']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS RELAT...  REL_MICRO_P  REL_MICRO_R  REL_MICRO_F  SCORE \n",
            "---  ------  -------------  -------------  -----------  -----------  -----------  ------\n",
            "  0       0           0.01           0.35         3.51        50.19         6.56    0.07\n",
            "  0     100           4.71           6.36        65.69        39.11        49.02    0.49\n",
            "  0     200           0.51           1.79        63.50        58.56        60.93    0.61\n",
            "  0     300           0.32           1.64        96.30        10.12        18.31    0.18\n",
            "  1     400           0.34           1.53        89.07        42.80        57.82    0.58\n",
            "  1     500           0.37           1.43        78.32        56.23        65.46    0.65\n",
            "  1     600           0.28           1.42        83.23        53.11        64.85    0.65\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "models/distilroberta-base-ner-conll2003_withnegatives/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env TRF_PATH=\"deepset/minilm-uncased-squad2\"\n",
        "%env MODEL_STRING = \"minilm-uncased-squad2_withnegatives\"\n",
        "%env TRAIN_BIN=\"train_withnegatives.spacy\"\n",
        "%env DEV_BIN=\"dev.spacy\"\n",
        "%env TEST_BIN=\"test.spacy\"\n",
        "\n",
        "!spacy project run train "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PF7_bkMC5DW",
        "outputId": "0951b252-f15c-467e-a7f3-c81079f32300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TRF_PATH=\"deepset/minilm-uncased-squad2\"\n",
            "env: MODEL_STRING=\"minilm-uncased-squad2_withnegatives\"\n",
            "env: TRAIN_BIN=\"train_withnegatives.spacy\"\n",
            "env: DEV_BIN=\"dev.spacy\"\n",
            "env: TEST_BIN=\"test.spacy\"\n",
            "\u001b[1m\n",
            "=================================== train ===================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy train configs/rel_trf.cfg --output models/minilm-uncased-squad2_withnegatives --components.transformer.model.name deepset/minilm-uncased-squad2 --paths.train data/train_withnegatives.spacy --paths.dev data/dev.spacy -c ./scripts/custom_functions.py --gpu-id 0\n",
            "\u001b[38;5;4mℹ Saving to output directory:\n",
            "models/minilm-uncased-squad2_withnegatives\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-06-20 14:01:54,018] [INFO] Set up nlp object from config\n",
            "[2022-06-20 14:01:54,030] [INFO] Pipeline: ['transformer', 'relation_extractor']\n",
            "[2022-06-20 14:01:54,035] [INFO] Created vocabulary\n",
            "[2022-06-20 14:01:54,037] [INFO] Finished initializing nlp object\n",
            "Some weights of the model checkpoint at deepset/minilm-uncased-squad2 were not used when initializing BertModel: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2022-06-20 14:02:04,700] [INFO] Initialized pipeline components: ['transformer', 'relation_extractor']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'relation_extractor']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS RELAT...  REL_MICRO_P  REL_MICRO_R  REL_MICRO_F  SCORE \n",
            "---  ------  -------------  -------------  -----------  -----------  -----------  ------\n",
            "  0       0           0.01           0.72         3.62       100.00         6.99    0.07\n",
            "  0     100           8.92          17.02         0.00         0.00         0.00    0.00\n",
            "  0     200           0.55           2.00        87.10        31.52        46.29    0.46\n",
            "  0     300           0.35           1.54       100.00         7.39        13.77    0.14\n",
            "  1     400           0.39           1.52        89.09        38.13        53.41    0.53\n",
            "  1     500           0.42           1.38        84.64        50.39        63.17    0.63\n",
            "  1     600           0.36           1.43        85.67        48.83        62.21    0.62\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "models/minilm-uncased-squad2_withnegatives/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Reflection**"
      ],
      "metadata": {
        "id": "d2FbzgItnFh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- I guess my hypothesis was incorrect <br>\n",
        "\n",
        "  - given the size of the dataset, adding adversarial examples seems to have \"confused\" the model <br>\n",
        "\n",
        "- an NER component trained on a large corpus to identify only PERSON and ORGANIZATION tags might allow further improvement <br>\n",
        "\n",
        "- training a model from scratch with classic NSP/MLM training objectives, but on a corpus like TACRED, might also yield further improvements <br>\n",
        "\n",
        "- restricting candidates by accounting for dependency relations heuristically might also be useful... <br>\n",
        "\n",
        "  - eg. not all entity pairs in a given example are related, or a person/org might exist in separate clauses and thus should not be considered"
      ],
      "metadata": {
        "id": "RelWf8SSySNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subtask 5 - Test Set Inference"
      ],
      "metadata": {
        "id": "u7NFDokvnLwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code**"
      ],
      "metadata": {
        "id": "hwr1lu8GzYnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env TRF_PATH=\"philschmid/distilroberta-base-ner-conll2003\"\n",
        "%env MODEL_STRING =\"distilroberta-base-ner-conll2003\"\n",
        "%env TRAIN_BIN=\"train.spacy\"\n",
        "%env DEV_BIN=\"dev.spacy\"\n",
        "%env TEST_BIN=\"test.spacy\"\n",
        "\n",
        "!spacy project run evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEpsr80xJO9L",
        "outputId": "14de0d27-578e-47f9-a11e-5e1e3e3c1c4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TRF_PATH=\"philschmid/distilroberta-base-ner-conll2003\"\n",
            "env: MODEL_STRING=\"distilroberta-base-ner-conll2003\"\n",
            "env: TRAIN_BIN=\"train.spacy\"\n",
            "env: DEV_BIN=\"dev.spacy\"\n",
            "env: TEST_BIN=\"test.spacy\"\n",
            "\u001b[1m\n",
            "================================== evaluate ==================================\u001b[0m\n",
            "Running command: /usr/bin/python3 ./scripts/evaluate.py models/distilroberta-base-ner-conll2003/model-best data/test.spacy False\n",
            "\n",
            "Random baseline:\n",
            "threshold 0.00 \t {'rel_micro_p': '3.50', 'rel_micro_r': '100.00', 'rel_micro_f': '6.77'}\n",
            "threshold 0.05 \t {'rel_micro_p': '3.51', 'rel_micro_r': '95.09', 'rel_micro_f': '6.77'}\n",
            "threshold 0.10 \t {'rel_micro_p': '3.54', 'rel_micro_r': '90.75', 'rel_micro_f': '6.81'}\n",
            "threshold 0.20 \t {'rel_micro_p': '3.52', 'rel_micro_r': '80.35', 'rel_micro_f': '6.75'}\n",
            "threshold 0.30 \t {'rel_micro_p': '3.56', 'rel_micro_r': '71.39', 'rel_micro_f': '6.78'}\n",
            "threshold 0.40 \t {'rel_micro_p': '3.54', 'rel_micro_r': '61.27', 'rel_micro_f': '6.69'}\n",
            "threshold 0.50 \t {'rel_micro_p': '3.37', 'rel_micro_r': '48.55', 'rel_micro_f': '6.31'}\n",
            "threshold 0.60 \t {'rel_micro_p': '3.44', 'rel_micro_r': '39.60', 'rel_micro_f': '6.32'}\n",
            "threshold 0.70 \t {'rel_micro_p': '3.53', 'rel_micro_r': '30.64', 'rel_micro_f': '6.32'}\n",
            "threshold 0.80 \t {'rel_micro_p': '3.74', 'rel_micro_r': '21.68', 'rel_micro_f': '6.38'}\n",
            "threshold 0.90 \t {'rel_micro_p': '3.35', 'rel_micro_r': '9.54', 'rel_micro_f': '4.95'}\n",
            "threshold 0.99 \t {'rel_micro_p': '3.88', 'rel_micro_r': '1.16', 'rel_micro_f': '1.78'}\n",
            "threshold 1.00 \t {'rel_micro_p': '0.00', 'rel_micro_r': '0.00', 'rel_micro_f': '0.00'}\n",
            "\n",
            "Results of the trained model:\n",
            "threshold 0.00 \t {'rel_micro_p': '3.50', 'rel_micro_r': '100.00', 'rel_micro_f': '6.77'}\n",
            "threshold 0.05 \t {'rel_micro_p': '51.53', 'rel_micro_r': '87.86', 'rel_micro_f': '64.96'}\n",
            "threshold 0.10 \t {'rel_micro_p': '56.79', 'rel_micro_r': '86.99', 'rel_micro_f': '68.72'}\n",
            "threshold 0.20 \t {'rel_micro_p': '63.11', 'rel_micro_r': '82.08', 'rel_micro_f': '71.36'}\n",
            "threshold 0.30 \t {'rel_micro_p': '67.33', 'rel_micro_r': '78.61', 'rel_micro_f': '72.53'}\n",
            "threshold 0.40 \t {'rel_micro_p': '70.38', 'rel_micro_r': '74.86', 'rel_micro_f': '72.55'}\n",
            "threshold 0.50 \t {'rel_micro_p': '73.35', 'rel_micro_r': '70.81', 'rel_micro_f': '72.06'}\n",
            "threshold 0.60 \t {'rel_micro_p': '77.05', 'rel_micro_r': '65.03', 'rel_micro_f': '70.53'}\n",
            "threshold 0.70 \t {'rel_micro_p': '79.76', 'rel_micro_r': '56.94', 'rel_micro_f': '66.44'}\n",
            "threshold 0.80 \t {'rel_micro_p': '83.76', 'rel_micro_r': '47.69', 'rel_micro_f': '60.77'}\n",
            "threshold 0.90 \t {'rel_micro_p': '87.22', 'rel_micro_r': '33.53', 'rel_micro_f': '48.43'}\n",
            "threshold 0.99 \t {'rel_micro_p': '0.00', 'rel_micro_r': '0.00', 'rel_micro_f': '0.00'}\n",
            "threshold 1.00 \t {'rel_micro_p': '0.00', 'rel_micro_r': '0.00', 'rel_micro_f': '0.00'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env TRF_PATH=\"deepset/minilm-uncased-squad2\"\n",
        "%env MODEL_STRING = \"minilm-uncased-squad2\"\n",
        "%env TRAIN_BIN=\"train.spacy\"\n",
        "%env DEV_BIN=\"dev.spacy\"\n",
        "%env TEST_BIN=\"test.spacy\"\n",
        "\n",
        "!spacy project run evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EThU_cSPDKbT",
        "outputId": "a73586af-a1e9-45b7-c46c-6ca8b8db90e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TRF_PATH=\"deepset/minilm-uncased-squad2\"\n",
            "env: MODEL_STRING=\"minilm-uncased-squad2\"\n",
            "env: TRAIN_BIN=\"train.spacy\"\n",
            "env: DEV_BIN=\"dev.spacy\"\n",
            "env: TEST_BIN=\"test.spacy\"\n",
            "\u001b[1m\n",
            "================================== evaluate ==================================\u001b[0m\n",
            "Running command: /usr/bin/python3 ./scripts/evaluate.py models/minilm-uncased-squad2/model-best data/test.spacy False\n",
            "\n",
            "Random baseline:\n",
            "threshold 0.00 \t {'rel_micro_p': '3.50', 'rel_micro_r': '100.00', 'rel_micro_f': '6.77'}\n",
            "threshold 0.05 \t {'rel_micro_p': '3.52', 'rel_micro_r': '95.66', 'rel_micro_f': '6.79'}\n",
            "threshold 0.10 \t {'rel_micro_p': '3.55', 'rel_micro_r': '91.33', 'rel_micro_f': '6.84'}\n",
            "threshold 0.20 \t {'rel_micro_p': '3.54', 'rel_micro_r': '80.92', 'rel_micro_f': '6.78'}\n",
            "threshold 0.30 \t {'rel_micro_p': '3.53', 'rel_micro_r': '70.23', 'rel_micro_f': '6.73'}\n",
            "threshold 0.40 \t {'rel_micro_p': '3.47', 'rel_micro_r': '59.54', 'rel_micro_f': '6.56'}\n",
            "threshold 0.50 \t {'rel_micro_p': '3.46', 'rel_micro_r': '49.71', 'rel_micro_f': '6.48'}\n",
            "threshold 0.60 \t {'rel_micro_p': '3.56', 'rel_micro_r': '40.46', 'rel_micro_f': '6.54'}\n",
            "threshold 0.70 \t {'rel_micro_p': '3.66', 'rel_micro_r': '31.21', 'rel_micro_f': '6.56'}\n",
            "threshold 0.80 \t {'rel_micro_p': '3.78', 'rel_micro_r': '21.39', 'rel_micro_f': '6.42'}\n",
            "threshold 0.90 \t {'rel_micro_p': '4.43', 'rel_micro_r': '12.72', 'rel_micro_f': '6.57'}\n",
            "threshold 0.99 \t {'rel_micro_p': '3.92', 'rel_micro_r': '1.16', 'rel_micro_f': '1.79'}\n",
            "threshold 1.00 \t {'rel_micro_p': '8.33', 'rel_micro_r': '0.29', 'rel_micro_f': '0.56'}\n",
            "\n",
            "Results of the trained model:\n",
            "threshold 0.00 \t {'rel_micro_p': '3.50', 'rel_micro_r': '100.00', 'rel_micro_f': '6.77'}\n",
            "threshold 0.05 \t {'rel_micro_p': '54.70', 'rel_micro_r': '90.75', 'rel_micro_f': '68.26'}\n",
            "threshold 0.10 \t {'rel_micro_p': '64.43', 'rel_micro_r': '85.84', 'rel_micro_f': '73.61'}\n",
            "threshold 0.20 \t {'rel_micro_p': '72.40', 'rel_micro_r': '80.35', 'rel_micro_f': '76.16'}\n",
            "threshold 0.30 \t {'rel_micro_p': '76.20', 'rel_micro_r': '77.75', 'rel_micro_f': '76.97'}\n",
            "threshold 0.40 \t {'rel_micro_p': '78.02', 'rel_micro_r': '72.83', 'rel_micro_f': '75.34'}\n",
            "threshold 0.50 \t {'rel_micro_p': '81.21', 'rel_micro_r': '69.94', 'rel_micro_f': '75.16'}\n",
            "threshold 0.60 \t {'rel_micro_p': '84.95', 'rel_micro_r': '68.50', 'rel_micro_f': '75.84'}\n",
            "threshold 0.70 \t {'rel_micro_p': '87.40', 'rel_micro_r': '64.16', 'rel_micro_f': '74.00'}\n",
            "threshold 0.80 \t {'rel_micro_p': '90.18', 'rel_micro_r': '58.38', 'rel_micro_f': '70.88'}\n",
            "threshold 0.90 \t {'rel_micro_p': '94.34', 'rel_micro_r': '43.35', 'rel_micro_f': '59.41'}\n",
            "threshold 0.99 \t {'rel_micro_p': '100.00', 'rel_micro_r': '0.29', 'rel_micro_f': '0.58'}\n",
            "threshold 1.00 \t {'rel_micro_p': '0.00', 'rel_micro_r': '0.00', 'rel_micro_f': '0.00'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env TRF_PATH=\"philschmid/distilroberta-base-ner-conll2003\"\n",
        "%env MODEL_STRING =\"distilroberta-base-ner-conll2003_withnegatives\"\n",
        "%env TRAIN_BIN=\"train_withnegatives.spacy\"\n",
        "%env DEV_BIN=\"dev.spacy\"\n",
        "%env TEST_BIN=\"test.spacy\"\n",
        "\n",
        "!spacy project run evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MRWFNy_JYVS",
        "outputId": "c658dd2c-4cc2-4fe0-f47f-7773e6ea57eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TRF_PATH=\"philschmid/distilroberta-base-ner-conll2003\"\n",
            "env: MODEL_STRING=\"distilroberta-base-ner-conll2003_withnegatives\"\n",
            "env: TRAIN_BIN=\"train_withnegatives.spacy\"\n",
            "env: DEV_BIN=\"dev.spacy\"\n",
            "env: TEST_BIN=\"test.spacy\"\n",
            "\u001b[1m\n",
            "================================== evaluate ==================================\u001b[0m\n",
            "Running command: /usr/bin/python3 ./scripts/evaluate.py models/distilroberta-base-ner-conll2003_withnegatives/model-best data/test.spacy False\n",
            "\n",
            "Random baseline:\n",
            "threshold 0.00 \t {'rel_micro_p': '3.50', 'rel_micro_r': '100.00', 'rel_micro_f': '6.77'}\n",
            "threshold 0.05 \t {'rel_micro_p': '3.55', 'rel_micro_r': '96.24', 'rel_micro_f': '6.85'}\n",
            "threshold 0.10 \t {'rel_micro_p': '3.58', 'rel_micro_r': '92.20', 'rel_micro_f': '6.90'}\n",
            "threshold 0.20 \t {'rel_micro_p': '3.52', 'rel_micro_r': '80.06', 'rel_micro_f': '6.74'}\n",
            "threshold 0.30 \t {'rel_micro_p': '3.54', 'rel_micro_r': '70.23', 'rel_micro_f': '6.73'}\n",
            "threshold 0.40 \t {'rel_micro_p': '3.58', 'rel_micro_r': '61.56', 'rel_micro_f': '6.76'}\n",
            "threshold 0.50 \t {'rel_micro_p': '3.50', 'rel_micro_r': '50.00', 'rel_micro_f': '6.54'}\n",
            "threshold 0.60 \t {'rel_micro_p': '3.47', 'rel_micro_r': '39.60', 'rel_micro_f': '6.38'}\n",
            "threshold 0.70 \t {'rel_micro_p': '3.48', 'rel_micro_r': '29.77', 'rel_micro_f': '6.23'}\n",
            "threshold 0.80 \t {'rel_micro_p': '3.52', 'rel_micro_r': '20.52', 'rel_micro_f': '6.00'}\n",
            "threshold 0.90 \t {'rel_micro_p': '2.77', 'rel_micro_r': '8.09', 'rel_micro_f': '4.13'}\n",
            "threshold 0.99 \t {'rel_micro_p': '3.61', 'rel_micro_r': '0.87', 'rel_micro_f': '1.40'}\n",
            "threshold 1.00 \t {'rel_micro_p': '0.00', 'rel_micro_r': '0.00', 'rel_micro_f': '0.00'}\n",
            "\n",
            "Results of the trained model:\n",
            "threshold 0.00 \t {'rel_micro_p': '3.50', 'rel_micro_r': '100.00', 'rel_micro_f': '6.77'}\n",
            "threshold 0.05 \t {'rel_micro_p': '49.03', 'rel_micro_r': '87.86', 'rel_micro_f': '62.94'}\n",
            "threshold 0.10 \t {'rel_micro_p': '58.67', 'rel_micro_r': '84.10', 'rel_micro_f': '69.12'}\n",
            "threshold 0.20 \t {'rel_micro_p': '67.25', 'rel_micro_r': '78.32', 'rel_micro_f': '72.36'}\n",
            "threshold 0.30 \t {'rel_micro_p': '73.18', 'rel_micro_r': '72.54', 'rel_micro_f': '72.86'}\n",
            "threshold 0.40 \t {'rel_micro_p': '77.63', 'rel_micro_r': '66.18', 'rel_micro_f': '71.45'}\n",
            "threshold 0.50 \t {'rel_micro_p': '81.37', 'rel_micro_r': '61.85', 'rel_micro_f': '70.28'}\n",
            "threshold 0.60 \t {'rel_micro_p': '85.90', 'rel_micro_r': '56.36', 'rel_micro_f': '68.06'}\n",
            "threshold 0.70 \t {'rel_micro_p': '88.30', 'rel_micro_r': '47.98', 'rel_micro_f': '62.17'}\n",
            "threshold 0.80 \t {'rel_micro_p': '89.23', 'rel_micro_r': '33.53', 'rel_micro_f': '48.74'}\n",
            "threshold 0.90 \t {'rel_micro_p': '100.00', 'rel_micro_r': '9.54', 'rel_micro_f': '17.41'}\n",
            "threshold 0.99 \t {'rel_micro_p': '0.00', 'rel_micro_r': '0.00', 'rel_micro_f': '0.00'}\n",
            "threshold 1.00 \t {'rel_micro_p': '0.00', 'rel_micro_r': '0.00', 'rel_micro_f': '0.00'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env TRF_PATH=\"deepset/minilm-uncased-squad2\"\n",
        "%env MODEL_STRING = \"minilm-uncased-squad2_withnegatives\"\n",
        "%env TRAIN_BIN=\"train_withnegatives.spacy\"\n",
        "%env DEV_BIN=\"dev.spacy\"\n",
        "%env TEST_BIN=\"test.spacy\"\n",
        "\n",
        "!spacy project run evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXOSP5bmDegZ",
        "outputId": "1cfca10f-b642-4641-84d7-d4e24db50f75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TRF_PATH=\"deepset/minilm-uncased-squad2\"\n",
            "env: MODEL_STRING=\"minilm-uncased-squad2_withnegatives\"\n",
            "env: TRAIN_BIN=\"train_withnegatives.spacy\"\n",
            "env: DEV_BIN=\"dev.spacy\"\n",
            "env: TEST_BIN=\"test.spacy\"\n",
            "\u001b[1m\n",
            "================================== evaluate ==================================\u001b[0m\n",
            "Running command: /usr/bin/python3 ./scripts/evaluate.py models/minilm-uncased-squad2_withnegatives/model-best data/test.spacy False\n",
            "\n",
            "Random baseline:\n",
            "threshold 0.00 \t {'rel_micro_p': '3.50', 'rel_micro_r': '100.00', 'rel_micro_f': '6.77'}\n",
            "threshold 0.05 \t {'rel_micro_p': '3.46', 'rel_micro_r': '94.22', 'rel_micro_f': '6.67'}\n",
            "threshold 0.10 \t {'rel_micro_p': '3.47', 'rel_micro_r': '89.31', 'rel_micro_f': '6.68'}\n",
            "threshold 0.20 \t {'rel_micro_p': '3.39', 'rel_micro_r': '77.75', 'rel_micro_f': '6.49'}\n",
            "threshold 0.30 \t {'rel_micro_p': '3.35', 'rel_micro_r': '67.05', 'rel_micro_f': '6.38'}\n",
            "threshold 0.40 \t {'rel_micro_p': '3.42', 'rel_micro_r': '58.38', 'rel_micro_f': '6.46'}\n",
            "threshold 0.50 \t {'rel_micro_p': '3.39', 'rel_micro_r': '48.27', 'rel_micro_f': '6.33'}\n",
            "threshold 0.60 \t {'rel_micro_p': '3.32', 'rel_micro_r': '37.86', 'rel_micro_f': '6.11'}\n",
            "threshold 0.70 \t {'rel_micro_p': '3.16', 'rel_micro_r': '26.88', 'rel_micro_f': '5.65'}\n",
            "threshold 0.80 \t {'rel_micro_p': '3.01', 'rel_micro_r': '17.34', 'rel_micro_f': '5.13'}\n",
            "threshold 0.90 \t {'rel_micro_p': '3.49', 'rel_micro_r': '9.83', 'rel_micro_f': '5.16'}\n",
            "threshold 0.99 \t {'rel_micro_p': '2.97', 'rel_micro_r': '0.87', 'rel_micro_f': '1.34'}\n",
            "threshold 1.00 \t {'rel_micro_p': '10.00', 'rel_micro_r': '0.29', 'rel_micro_f': '0.56'}\n",
            "\n",
            "Results of the trained model:\n",
            "threshold 0.00 \t {'rel_micro_p': '3.50', 'rel_micro_r': '100.00', 'rel_micro_f': '6.77'}\n",
            "threshold 0.05 \t {'rel_micro_p': '53.06', 'rel_micro_r': '87.57', 'rel_micro_f': '66.09'}\n",
            "threshold 0.10 \t {'rel_micro_p': '66.44', 'rel_micro_r': '82.95', 'rel_micro_f': '73.78'}\n",
            "threshold 0.20 \t {'rel_micro_p': '77.81', 'rel_micro_r': '76.01', 'rel_micro_f': '76.90'}\n",
            "threshold 0.30 \t {'rel_micro_p': '81.37', 'rel_micro_r': '71.97', 'rel_micro_f': '76.38'}\n",
            "threshold 0.40 \t {'rel_micro_p': '84.12', 'rel_micro_r': '67.34', 'rel_micro_f': '74.80'}\n",
            "threshold 0.50 \t {'rel_micro_p': '87.50', 'rel_micro_r': '62.72', 'rel_micro_f': '73.06'}\n",
            "threshold 0.60 \t {'rel_micro_p': '88.94', 'rel_micro_r': '55.78', 'rel_micro_f': '68.56'}\n",
            "threshold 0.70 \t {'rel_micro_p': '91.71', 'rel_micro_r': '47.98', 'rel_micro_f': '63.00'}\n",
            "threshold 0.80 \t {'rel_micro_p': '96.27', 'rel_micro_r': '37.28', 'rel_micro_f': '53.75'}\n",
            "threshold 0.90 \t {'rel_micro_p': '100.00', 'rel_micro_r': '11.56', 'rel_micro_f': '20.73'}\n",
            "threshold 0.99 \t {'rel_micro_p': '0.00', 'rel_micro_r': '0.00', 'rel_micro_f': '0.00'}\n",
            "threshold 1.00 \t {'rel_micro_p': '0.00', 'rel_micro_r': '0.00', 'rel_micro_f': '0.00'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "\n",
        "The adversarial-trained model actually matched the performance of the Iteration 2 model, albeit with a very low threshold - this makes sense because by adding adversarial examples, we forced model weights to be more discriminative. \n",
        "\n",
        "In practice, performance will be better because we rely on a better NER tagger for candidate entity pairs. "
      ],
      "metadata": {
        "id": "VJ2c2ngig92u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Base Model  | Threshold | Micro-Precision | Micro-Recall | Micro-F1 \n",
        "------------|-----------|-----------------|--------------|--------------\n",
        "distilroberta-base-ner-conll2003 | 0.4 | 70.4 | 74.9 | 72.6\n",
        "**minilm-uncased-squad2** | **0.3** | **76.2** | **77.8** | **77.0**\n",
        "distilroberta-base-ner-conll2003 * | 0.3 | 73.2 | 72.5 | 72.9\n",
        "minilm-uncased-squad2 * | 0.2 | 77.8 | 76.01 | 76.9\n"
      ],
      "metadata": {
        "id": "Y4gN3-JtepgV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo"
      ],
      "metadata": {
        "id": "-fl1qdKS1nhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!pip install -U pip setuptools wheel\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_trf\n",
        "!pip install spacy transformers"
      ],
      "metadata": {
        "id": "WX5Xe7W61piB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!spacy project run clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FA0nlQZcp0cG",
        "outputId": "44e1b4b5-d9e2-49b2-fc84-68d2d6a35068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "=================================== clean ===================================\u001b[0m\n",
            "Running command: rm -rf assets/sentences.spacy\n",
            "Running command: rm -rf 'assets/output/*'\n",
            "Running command: rm -rf training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env TRF_PATH=\"deepset/minilm-uncased-squad2\"\n",
        "%env MODEL_STRING = \"minilm-uncased-squad2\" # best model \n",
        "!spacy project run infer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3l2RkwAjThQ",
        "outputId": "77d0951e-b1ee-4449-be58-7f88a9b16f17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TRF_PATH=\"deepset/minilm-uncased-squad2\"\n",
            "env: MODEL_STRING=\"minilm-uncased-squad2\"\n",
            "\u001b[1m\n",
            "=================================== infer ===================================\u001b[0m\n",
            "Running command: /usr/bin/python3 ./scripts/inference.py assets/sentences.txt models/minilm-uncased-squad2/model-best assets/output/extracted_sentences.txt\n",
            "\u001b[38;5;4mℹ {'index': 0, 'person_and_company': ('Daniel Spielman', 'Yale\n",
            "University')}\u001b[0m\n",
            "\u001b[38;5;4mℹ {'index': 1, 'person_and_company': ('Spielman', 'MIT')}\u001b[0m\n",
            "\u001b[38;5;4mℹ {'index': 1, 'person_and_company': ('Shang-Hua Teng', 'MIT')}\u001b[0m\n",
            "\u001b[38;5;4mℹ {'index': 3, 'person_and_company': ('Joanna Drążkowska', 'Ludwig\n",
            "Maximilian University of Munich')}\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}